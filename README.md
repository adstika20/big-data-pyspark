# Pengolahan Big Data dengan PySpark

## Apa Itu Big Data?
Big Data adalah kumpulan data yang memiliki karakteristik **3V**:

- **Volume**: Ukuran data sangat besar   
- **Velocity**: Data dihasilkan dan diproses dengan sangat cepat 
- **Variety**: Data berasal dari berbagai sumber dan dalam berbagai format 

Karena skala dan kompleksitasnya, big data tidak bisa diproses dengan alat tradisional. Dibutuhkan teknologi khusus untuk menangani tantangan ini.

---

## Kenalan dengan Apache Spark
[Apache Spark](https://spark.apache.org/) adalah engine pemrosesan data terdistribusi yang dirancang untuk:

- **Cepat**
- **Fleksibel**
- **Skalabel**

### Fitur Unggulan Apache Spark:
- Eksekusi **in-memory**
- **Toleransi terhadap kegagalan**
- Mendukung berbagai bahasa: **Scala**, **Java**, **R**, dan **Python**

---

## 🐍✨ PySpark = Spark + Python
**PySpark** adalah antarmuka **Python API** untuk Apache Spark.

### 💡 PySpark Cocok Untuk:
- Data Engineer & Data Scientist 
- Proses **ETL (Extract, Transform, Load)** 
- **Machine Learning** di skala besar 

---

## Arsitektur Spark (Sederhana) 
Berikut adalah komponen utama dalam arsitektur Spark:

- **Driver Program**: Mengontrol eksekusi aplikasi Spark
- **Cluster Manager**: Mengelola dan menyediakan sumber daya untuk Spark
- **Worker Node**: Menjalankan tugas-tugas yang diberikan oleh driver

**Data** dibagi menjadi **partisi** dan diproses secara **paralel**, meningkatkan efisiensi pemrosesan data besar.

---

